# üö¶ An√°lise de Acidentes de Tr√¢nsito em Salvador (2025)

Projeto de **Engenharia de Dados** com foco na **an√°lise dos acidentes de tr√¢nsito ocorridos em Salvador (BA) no ano de 2025**, utilizando pipelines orquestradas com **Apache Airflow**, ambiente conteinerizado com **Docker**, persist√™ncia em **PostgreSQL** e disponibiliza√ß√£o dos dados via **Google Sheets**.


## üéØ Objetivo do Projeto

O objetivo principal deste projeto √© **analisar os acidentes de tr√¢nsito em Salvador no ano de 2025**, transformando dados brutos em uma base estruturada, confi√°vel e pronta para an√°lise.

Para atingir esse objetivo, foi desenvolvida uma arquitetura de dados que:

- Ingiere dados brutos de acidentes
- Realiza transforma√ß√µes e modelagem dos dados
- Armazena os dados tratados em um banco relacional
- Orquestra todo o fluxo com Airflow
- Disponibiliza os dados finais para consumo anal√≠tico

### üîé Exemplos de an√°lises poss√≠veis
- Tipos de acidentes mais frequentes em Salvador em 2025
- Hor√°rios e dias da semana com maior incid√™ncia
- Rela√ß√£o entre condi√ß√£o meteorol√≥gica e ocorr√™ncia de acidentes
- Impacto da visibilidade e fase do dia
- Distribui√ß√£o de acidentes por localiza√ß√£o


## üß† Contexto do Problema

Dados de acidentes de tr√¢nsito geralmente s√£o disponibilizados de forma bruta, dificultando an√°lises diretas.  
Este projeto resolve esse problema ao aplicar pr√°ticas de **engenharia de dados**, garantindo:

- Reprodutibilidade
- Padroniza√ß√£o
- Orquestra√ß√£o
- Facilidade de consumo dos dados


## üß± Arquitetura da Solu√ß√£o

- **PostgreSQL**: Armazenamento dos dados tratados
- **Apache Airflow**: Orquestra√ß√£o dos pipelines
- **Docker Compose**: Padroniza√ß√£o do ambiente
- **Google Sheets API**: Camada final de visualiza√ß√£o e compartilhamento

Fluxo de dados:

Dados Brutos ‚Üí PostgreSQL ‚Üí Airflow (ETL) ‚Üí Google Sheets


## üîÑ Pipeline de Dados

### DAG: `Analise_Acidentes_Salvador`

#### 1Ô∏è‚É£ Extract
- Consulta SQL no PostgreSQL
- Filtragem por munic√≠pio (Salvador) e UF (BA)
- Retorno dos dados via XCom em formato JSON

#### 2Ô∏è‚É£ Load
- Reconstru√ß√£o do DataFrame a partir do XCom
- Autentica√ß√£o via Service Account (Google Sheets)
- Escrita dos dados na planilha final

> N√£o h√° gera√ß√£o de arquivos intermedi√°rios (CSV ou Parquet).  
> A comunica√ß√£o entre as tasks ocorre via **XCom**.


## ‚öôÔ∏è Tecnologias Utilizadas

- Python
- Pandas
- SQL
- PostgreSQL
- SQLAlchemy
- Apache Airflow
- Docker 
- Google Sheets API

## ‚ñ∂Ô∏è Como Executar o Projeto

### Pr√©-requisitos
- Docker
- Docker Compose
- Credenciais do Google Sheets (Service Account)

### Execu√ß√£o


docker compose up -d
Acesse a interface do Airflow:

http://localhost:8089
üîê Vari√°veis de Ambiente
As vari√°veis sens√≠veis n√£o s√£o versionadas.
Exemplo de .env utilizado no projeto:

DB_USER=airflow
DB_PASSWORD=airflow
DB_HOST=postgres
DB_PORT=5432
DB_NAME=registro_acidentes

PATH_PRF=/opt/airflow/data/datatran2025.csv
As credenciais do Google Sheets s√£o montadas via volume Docker e referenciadas por vari√°vel de ambiente.


## üìå Boas Pr√°ticas Aplicadas

Separa√ß√£o clara entre DAGs e l√≥gica de pipeline

Uso correto de XCom (sem arquivos intermedi√°rios)

Ambiente reprodut√≠vel com Docker

Prote√ß√£o de credenciais via .gitignore

Projeto preparado para m√∫ltiplas DAGs


## üöÄ Poss√≠veis Evolu√ß√µes

Persist√™ncia em Data Lake (S3 / GCS)

Uso de formato Parquet

Cria√ß√£o de dashboards (Looker / Power BI)

Integra√ß√£o com BigQuery

Monitoramento e alertas no Airflow